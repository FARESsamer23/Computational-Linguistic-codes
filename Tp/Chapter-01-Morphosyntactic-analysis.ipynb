{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Morphological analysis\n",
    "\n",
    "One of the first tasks in morphological analysis is to detect the language of the message. This stage generally underpins most of the processing that can be carried out at higher levels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Premier exemple:  en\n",
      "Deuxième exemple:  de\n"
     ]
    }
   ],
   "source": [
    "#A simple example of an API that detects language is as follows\n",
    "\n",
    "from langdetect import detect, DetectorFactory #langdetect supports over 55 languages\n",
    "DetectorFactory.seed = 0 \n",
    "\n",
    "print(\"Premier exemple: \", detect(\"War doesn't show who's right, just who's left.\"))\n",
    "\n",
    "print(\"Deuxième exemple: \", detect(\"Ein, zwei, drei, vier\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More details on this library [here](https://github.com/Mimino666/langdetect/blob/master/README.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "\n",
    "In this section, we'll be looking at the **spaCy** library.\n",
    "\n",
    "Using [this page](https://applied-language-technology.mooc.fi/html/notebooks/part_ii/03_basic_nlp.html#morphological-analysis) as a reference, students are requested to:\n",
    "\n",
    "- Consider the following sentences:\n",
    "    1. \"إننا نتعلم من خلال الممارسة والخطأ\"\n",
    "    2. \"It’s by practicing and mistaking that we learn\"\n",
    "    3. \"C'est en pratiquant et en se trompant qu'on apprend\"\n",
    "    4. \"Es practicando y equivocándonos que aprendemos\"\n",
    "    5. \"我们通过练习和错误学习\"\n",
    "\n",
    "- For each of the sentences, you are asked to detect its language (using the Spacy library). Then for the sentence in English, you are asked to:\n",
    "\n",
    "    1. List the different morphemes present in the sentence\n",
    "    2. Specify the aspects of the third morpheme\n",
    "    3. Return a dictionary of this phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language for sentence: 'إننا نتعلم من خلال الممارسة والخطأ' is ar\n",
      "Detected language for sentence: 'It’s by practicing and mistaking that we learn' is en\n",
      "Morphemes in the English sentence: ['It', '’s', 'by', 'practicing', 'and', 'mistaking', 'that', 'we', 'learn']\n",
      "Aspects of the third morpheme: {'text': 'by', 'lemma': 'by', 'POS': 'ADP', 'dependency': 'prep', 'shape': 'xx', 'is_alpha': True, 'is_stop': True}\n",
      "Dictionary representation of the English sentence: {'It': {'lemma': 'it', 'POS': 'PRON', 'dependency': 'nsubj'}, '’s': {'lemma': '’', 'POS': 'VERB', 'dependency': 'ROOT'}, 'by': {'lemma': 'by', 'POS': 'ADP', 'dependency': 'prep'}, 'practicing': {'lemma': 'practice', 'POS': 'VERB', 'dependency': 'pcomp'}, 'and': {'lemma': 'and', 'POS': 'CCONJ', 'dependency': 'cc'}, 'mistaking': {'lemma': 'mistake', 'POS': 'VERB', 'dependency': 'conj'}, 'that': {'lemma': 'that', 'POS': 'PRON', 'dependency': 'dobj'}, 'we': {'lemma': 'we', 'POS': 'PRON', 'dependency': 'nsubj'}, 'learn': {'lemma': 'learn', 'POS': 'VERB', 'dependency': 'ccomp'}}\n",
      "Detected language for sentence: 'C'est en pratiquant et en se trompant qu'on apprend' is fr\n",
      "Detected language for sentence: 'Es practicando y equivocándonos que aprendemos' is es\n",
      "Detected language for sentence: '我们通过练习和错误学习' is zh-cn\n",
      "Results: {'English': {'morphemes': ['It', '’s', 'by', 'practicing', 'and', 'mistaking', 'that', 'we', 'learn'], 'third_morpheme_aspects': {'text': 'by', 'lemma': 'by', 'POS': 'ADP', 'dependency': 'prep', 'shape': 'xx', 'is_alpha': True, 'is_stop': True}, 'sentence_dict': {'It': {'lemma': 'it', 'POS': 'PRON', 'dependency': 'nsubj'}, '’s': {'lemma': '’', 'POS': 'VERB', 'dependency': 'ROOT'}, 'by': {'lemma': 'by', 'POS': 'ADP', 'dependency': 'prep'}, 'practicing': {'lemma': 'practice', 'POS': 'VERB', 'dependency': 'pcomp'}, 'and': {'lemma': 'and', 'POS': 'CCONJ', 'dependency': 'cc'}, 'mistaking': {'lemma': 'mistake', 'POS': 'VERB', 'dependency': 'conj'}, 'that': {'lemma': 'that', 'POS': 'PRON', 'dependency': 'dobj'}, 'we': {'lemma': 'we', 'POS': 'PRON', 'dependency': 'nsubj'}, 'learn': {'lemma': 'learn', 'POS': 'VERB', 'dependency': 'ccomp'}}}}\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from langdetect import detect\n",
    "\n",
    "# Load SpaCy's English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Sentences in different languages\n",
    "sentences = [\n",
    "    \"إننا نتعلم من خلال الممارسة والخطأ\",\n",
    "    \"It’s by practicing and mistaking that we learn\",\n",
    "    \"C'est en pratiquant et en se trompant qu'on apprend\",\n",
    "    \"Es practicando y equivocándonos que aprendemos\",\n",
    "    \"我们通过练习和错误学习\"\n",
    "]\n",
    "\n",
    "# Dictionary to store results\n",
    "results = {}\n",
    "\n",
    "# Loop through each sentence\n",
    "for sentence in sentences:\n",
    "    # Detect language\n",
    "    language = detect(sentence)\n",
    "    print(f\"Detected language for sentence: '{sentence}' is {language}\")\n",
    "\n",
    "    # Only process the English sentence for further analysis\n",
    "    if language == 'en':\n",
    "        # Process the sentence with SpaCy\n",
    "        doc = nlp(sentence)\n",
    "\n",
    "        # Extract morphemes (token text)\n",
    "        morphemes = [token.text for token in doc]\n",
    "        print(\"Morphemes in the English sentence:\", morphemes)\n",
    "\n",
    "        # Get the third morpheme and its aspects\n",
    "        if len(morphemes) >= 3:\n",
    "            third_morpheme = doc[2]\n",
    "            aspects = {\n",
    "                \"text\": third_morpheme.text,\n",
    "                \"lemma\": third_morpheme.lemma_,\n",
    "                \"POS\": third_morpheme.pos_,\n",
    "                \"dependency\": third_morpheme.dep_,\n",
    "                \"shape\": third_morpheme.shape_,\n",
    "                \"is_alpha\": third_morpheme.is_alpha,\n",
    "                \"is_stop\": third_morpheme.is_stop\n",
    "            }\n",
    "            print(\"Aspects of the third morpheme:\", aspects)\n",
    "\n",
    "        # Create a dictionary representation of the sentence\n",
    "        sentence_dict = {token.text: {\"lemma\": token.lemma_, \"POS\": token.pos_, \"dependency\": token.dep_} for token in doc}\n",
    "        print(\"Dictionary representation of the English sentence:\", sentence_dict)\n",
    "\n",
    "        # Store the results for the English sentence\n",
    "        results[\"English\"] = {\n",
    "            \"morphemes\": morphemes,\n",
    "            \"third_morpheme_aspects\": aspects,\n",
    "            \"sentence_dict\": sentence_dict\n",
    "        }\n",
    "\n",
    "print(\"Results:\", results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, these same tasks can be performed with a dedicated Linux library called Polyglot, which can perform the tasks described above and many others: [(Find some examples of: Tokenization, Part of speech tagging, Named Entity recognition, polarity detection, Embeddings, Transliteration)](https://pypi.org/project/polyglot/)[or this site, which offers the same tasks:](https://www.geeksforgeeks.org/natural-language-processing-using-polyglot-introduction/).Finally, we can go even further with the \"Morfessor\". [See this link](https://polyglot.readthedocs.io/en/latest/MorphologicalAnalysis.html) and [ this one here](http://aayushsanghavi.blogspot.com/2018/03/morphological-segmentation-of-words.html) also."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Syntactic analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Syntactic analysis with NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first introductory section, we will first look at a simple example of syntax tree generation using the NLTK library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "regles_grammaire = nltk.CFG.fromstring(\"\"\"\n",
    " S -> NP VP\n",
    " PP -> P NP\n",
    " NP -> Det N | Det N PP | 'I'\n",
    " VP -> V NP | VP PP\n",
    " Det -> 'a' | 'my'\n",
    " N -> 'mouse' | 'closet'\n",
    " V -> 'found'\n",
    " P -> 'in'\n",
    " \"\"\")\n",
    "\n",
    "sent = ['I', 'found', 'a', 'mouse', 'in', 'my', 'closet']\n",
    "parser = nltk.ChartParser(regles_grammaire)\n",
    "for tree in parser.parse(sent):\n",
    "    print(tree)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go further (by exploring the notion of *context free gramar*) with [this example](https://www.nltk.org/book/ch08.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "Taking the previous example, you are asked to change the rules base to take into account the following sentences:\n",
    "\n",
    "- I found a mouse in my closet \n",
    "- The fox jumps over the dog\n",
    "- I bought a toy for my son\n",
    "- I am in the classroom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copy your solution here\n",
    "#Let's do it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation of acquired libraries\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk import pos_tag, word_tokenize, RegexpParser\n",
    "\n",
    "# Text example\n",
    "sample_text = \"The quick brown fox jumps over the lazy dog\"\n",
    "\n",
    "# Find all the \"parts of speech\" tags in the given sentence\n",
    "tagged = pos_tag(word_tokenize(sample_text))\n",
    "\n",
    "# Extract all the parts of speech tags of any given text\n",
    "chunker = RegexpParser(\"\"\"NP: {<DT>?<JJ>*<NN>} #To extract Noun Phrases\n",
    "P: {<IN>} #To extract Prepositions\n",
    "V: {<V.*>} #To extract Verbs\n",
    "PP: {<p> <NP>} #To extract Prepositional Phrases\n",
    "VP: {<V> <NP|PP>*} #To extract Verb Phrases \"\"\")\n",
    "\n",
    "# Write all the POS tags of the given sentence\n",
    "output = chunker.parse(tagged)\n",
    "print(\"After Extracting\\n\", output)\n",
    "\n",
    "# Draw the tree (see the window that appears)\n",
    "output.draw()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can go further by exploring NLTK's Treebank library. Here is an introductory [link](https://www.nltk.org/) and a [video](https://www.youtube.com/watch?v=V19xvvjWPmE&ab_channel=HugoLarochelle) to give you a better idea. _**By the way, the guy from this channel has droped some other very interesting NLP videos online. Check them out ;)**_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 \n",
    "***\n",
    "\n",
    "Let's go back to our first link [spaCy](https://applied-language-technology.mooc.fi/html/notebooks/part_ii/03_basic_nlp.html#morphological-analysis). You are required to:\n",
    "- Display the syntax dependencies of the following quotations\n",
    "    1. Motivation is what gets you started, clothes is what keeps you going. ~Jim Ruyn_\n",
    "    2. Dream big and never give up\".\n",
    "- Display dependency trees (syntax trees) using **displacy de spaCy**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's do it\n",
    "#Copy your solution here\n",
    "#..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
